{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd3d62a5",
   "metadata": {},
   "source": [
    "# 00 Load and QC\n",
    "\n",
    "Load the latest collected CSVs, validate uniqueness, and filter degenerate runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adcd32cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T11:04:29.103223Z",
     "iopub.status.busy": "2026-01-02T11:04:29.103101Z",
     "iopub.status.idle": "2026-01-02T11:04:29.240030Z",
     "shell.execute_reply": "2026-01-02T11:04:29.239770Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stats: /home/gt/research/DiskANN/scripts/paramAnalysis/gridSearch/outputFiles/analyze/siftsmall01/collected_stats_siftsmall01_20260102_190313.csv\n",
      "topk : /home/gt/research/DiskANN/scripts/paramAnalysis/gridSearch/outputFiles/analyze/siftsmall01/collected_topk_siftsmall01_20260102_190313.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ANALYZE_DIR = Path('../outputFiles/analyze').resolve()\n",
    "REPORT_PREFIX = os.environ.get('REPORT_PREFIX', 'analysis_reports')\n",
    "COLLECT_PREFIX = os.environ.get('COLLECT_PREFIX', REPORT_PREFIX)\n",
    "REPORT_DIR = (ANALYZE_DIR / REPORT_PREFIX)\n",
    "COLLECT_DIR = (ANALYZE_DIR / COLLECT_PREFIX)\n",
    "STATS_CSV = None  # set to a specific file path if needed\n",
    "TOPK_CSV = None   # set to a specific file path if needed\n",
    "\n",
    "def pick_latest(pattern):\n",
    "    files = sorted(COLLECT_DIR.glob(pattern))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f'No files matched: {pattern}')\n",
    "    return files[-1]\n",
    "\n",
    "stats_path = Path(STATS_CSV) if STATS_CSV else pick_latest('collected_stats_*.csv')\n",
    "topk_path = Path(TOPK_CSV) if TOPK_CSV else pick_latest('collected_topk_*.csv')\n",
    "\n",
    "print('stats:', stats_path)\n",
    "print('topk :', topk_path)\n",
    "\n",
    "stats_df = pd.read_csv(stats_path)\n",
    "topk_df = pd.read_csv(topk_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faae4f6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T11:04:29.240983Z",
     "iopub.status.busy": "2026-01-02T11:04:29.240897Z",
     "iopub.status.idle": "2026-01-02T11:04:29.246395Z",
     "shell.execute_reply": "2026-01-02T11:04:29.246208Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows stats: 3150\n",
      "rows topk : 3150\n",
      "duplicate run_prefix in stats: 0\n",
      "runs with inconsistent search params: 0\n",
      "num_queries unique values: 1\n"
     ]
    }
   ],
   "source": [
    "# Basic integrity checks\n",
    "print('rows stats:', len(stats_df))\n",
    "print('rows topk :', len(topk_df))\n",
    "\n",
    "dup = stats_df['run_prefix'].duplicated().sum()\n",
    "print('duplicate run_prefix in stats:', dup)\n",
    "\n",
    "params_per_run = stats_df.groupby('run_prefix')[['search_L','search_W','search_K','search_T']].nunique()\n",
    "inconsistent = (params_per_run > 1).any(axis=1).sum()\n",
    "print('runs with inconsistent search params:', inconsistent)\n",
    "\n",
    "num_queries_unique = stats_df['num_queries'].nunique()\n",
    "print('num_queries unique values:', num_queries_unique)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72721565",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T11:04:29.247073Z",
     "iopub.status.busy": "2026-01-02T11:04:29.247004Z",
     "iopub.status.idle": "2026-01-02T11:04:29.254714Z",
     "shell.execute_reply": "2026-01-02T11:04:29.254457Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered rows: 3150\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_rows</th>\n",
       "      <th>degenerate_rows</th>\n",
       "      <th>low_recall_rows</th>\n",
       "      <th>filtered_rows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3150</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_rows  degenerate_rows  low_recall_rows  filtered_rows\n",
       "0        3150                0                0           3150"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Degenerate filters and recall thresholds\n",
    "recall_threshold = 0.7\n",
    "degenerate = (stats_df['search_L'] < stats_df['search_K']) | (stats_df['search_L'] < 2 * stats_df['search_W'])\n",
    "low_recall = stats_df['recall_mean'] < recall_threshold\n",
    "\n",
    "stats_df['flag_degenerate'] = degenerate\n",
    "stats_df['flag_low_recall'] = low_recall\n",
    "\n",
    "filtered = stats_df[~degenerate].copy()\n",
    "filtered = filtered[~low_recall].copy()\n",
    "\n",
    "print('filtered rows:', len(filtered))\n",
    "\n",
    "qc_summary = {\n",
    "    'total_rows': len(stats_df),\n",
    "    'degenerate_rows': int(degenerate.sum()),\n",
    "    'low_recall_rows': int(low_recall.sum()),\n",
    "    'filtered_rows': len(filtered),\n",
    "}\n",
    "qc_df = pd.DataFrame([qc_summary])\n",
    "qc_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8dc737b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T11:04:29.255491Z",
     "iopub.status.busy": "2026-01-02T11:04:29.255420Z",
     "iopub.status.idle": "2026-01-02T11:04:29.450190Z",
     "shell.execute_reply": "2026-01-02T11:04:29.449908Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /home/gt/research/DiskANN/scripts/paramAnalysis/gridSearch/outputFiles/analyze/siftsmall01/tables/qc_summary.csv\n",
      "Saved: /home/gt/research/DiskANN/scripts/paramAnalysis/gridSearch/outputFiles/analyze/siftsmall01/tables/filtered_stats.csv\n"
     ]
    }
   ],
   "source": [
    "# Save QC outputs\n",
    "out_tables = (REPORT_DIR / 'tables')\n",
    "out_tables.mkdir(parents=True, exist_ok=True)\n",
    "qc_df.to_csv(out_tables / 'qc_summary.csv', index=False)\n",
    "filtered.to_csv(out_tables / 'filtered_stats.csv', index=False)\n",
    "print('Saved:', out_tables / 'qc_summary.csv')\n",
    "print('Saved:', out_tables / 'filtered_stats.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
